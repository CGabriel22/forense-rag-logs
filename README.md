# Simplifica√ß√£o da an√°lise forense de logs utilizando Grandes Modelos de Linguagem com a t√©cnica RAG

Este reposit√≥rio cont√©m os artefatos do artigo "Simplifica√ß√£o da an√°lise forense de logs utilizando Grandes Modelos de Linguagem com a t√©cnica RAG", submetido ao Simp√≥sio Brasileiro de Ciberseguran√ßa (SBSeg).

O objetivo deste trabalho √© apresentar uma abordagem que utiliza Grandes Modelos de Linguagem (LLMs) combinados com a t√©cnica de Retrieval-Augmented Generation (RAG) para simplificar a an√°lise forense de logs de sistemas e redes. A solu√ß√£o proposta permite a detec√ß√£o, correla√ß√£o e interpreta√ß√£o de anomalias por meio de intera√ß√µes em linguagem natural, otimizando o processo de investiga√ß√£o e resposta a incidentes de seguran√ßa.

Abstract. In the digital age, the growing complexity of computer systems and the sophistication of cyberattacks significantly increase the volume of logs generated, posing challenges to cybersecurity professionals. Detecting and interpreting attacks or issues in these records are crucial for a swift response to security incidents. In this context, Large Language Models (LLMs) emerge as fundamental tools for understanding and generating natural language. This study presents an approach to analyze system and network logs, aiming to detect, correlate, and interpret anomalies using the Retrieval-Augmented Generation (RAG) technique with LLMs and interactions through targeted questions. The results demonstrate the effectiveness of the proposed approach in generating relevant insights and simplifying forensic analysis for professionals in the field.

Resumo. Na era digital, a crescente complexidade dos sistemas inform√°ticos e a sofistica√ß√£o dos ataques cibern√©ticos aumentam significativamente o volume de logs gerados, desafiando os profissionais de ciberseguran√ßa. A detec√ß√£o e interpreta√ß√£o de ataques ou problemas nesses registros s√£o essenciais para uma resposta r√°pida a incidentes de seguran√ßa. Nesse contexto, os Grandes Modelos de Linguagem (LLMs) destacam-se como ferramentas fundamentais na compreens√£o e gera√ß√£o de linguagem natural. Este estudo apresenta uma abordagem para analisar logs de sistemas e redes, visando detectar, correlacionar e interpretar anomalias, utilizando a t√©cnica de \emph{Retrieval-Augmented Generation} (RAG) com LLMs e intera√ß√µes por meio de perguntas direcionadas. Os resultados comprovam a efic√°cia da abordagem proposta em gerar informa√ß√µes relevantes e simplificar a an√°lise forense para profissionais da √°rea.

# Estrutura do readme.md

1. T√≠tulo do Projeto
2. Resumo e Abstract
3. Estrutura do README
4. Selos Considerados
5. Informa√ß√µes B√°sicas
    - Ambiente de desenvolvimento
    - Requisitos de hardware
    - Observa√ß√µes sobre desempenho
6. Depend√™ncias
7. Preocupa√ß√µes com Seguran√ßa
8. Instala√ß√£o
    - Instala√ß√£o do Python
    - Cria√ß√£o e ativa√ß√£o do ambiente virtual
    - Clonagem do reposit√≥rio
    - Instala√ß√£o de depend√™ncias
    - Instala√ß√£o e configura√ß√£o do Ollama
9. Teste M√≠nimo
    - Execu√ß√£o do Jupyter Notebook
    - Passo a passo para rodar o exemplo pr√°tico
    - Observa√ß√µes sobre datasets e poss√≠veis erros
10. Experimentos
11. Licen√ßa

# Selos Considerados

Os selos considerados s√£o: Artefatos Dispon√≠veis (SeloD), Artefatos Funcionais (SeloF), Artefatos Sustent√°veis (SeloS), Experimentos Reprodut√≠veis (SeloR).

# Informa√ß√µes b√°sicas

Este reposit√≥rio cont√©m os artefatos necess√°rios para a execu√ß√£o e replica√ß√£o dos experimentos descritos no artigo. 
Abaixo est√£o listados os requisitos m√≠nimos de hardware e software, bem como informa√ß√µes sobre o ambiente de desenvolvimento.

### Ambiente de desenvolvimento
- Sistema Operacional: Ubuntu 24.04 LTS (recomendado)
- Recomenda-se a utiliza√ß√£o de um ambiente Linux para melhor compatibilidade.

### Requisitos de hardware m√≠nimos
- Processador: Intel Core i3-9100 ou equivalente AMD
- Mem√≥ria RAM: 16 GB
- Placa de v√≠deo: NVIDIA RTX 2060
- Armazenamento: 5 GB de espa√ßo livre em disco
- Conex√£o com a internet: Banda larga (necess√°ria apenas durante o download e configura√ß√£o dos modelos e do sistema)

### Observa√ß√µes
- O uso de GPU √© recomendado para acelerar o processamento dos modelos de linguagem.
- Os testes e experimentos foram conduzidos utilizando uma configura√ß√£o com os requisitos acima e apresentaram desempenho satisfat√≥rio.

# Depend√™ncias
Para a execu√ß√£o correta dos experimentos e reprodutibilidade dos resultados, √© necess√°rio instalar as seguintes depend√™ncias e configurar os recursos de terceiros conforme descrito abaixo.

### Vers√£o do python
- Python 3 (testado com Python 3.10)

### Principais bibliotecas
As bibliotecas essenciais utilizadas no projeto incluem:
- langchain==0.2.16
- langchain-core==0.2.39
- langchain-community==0.2.16
- langchain-ollama==0.1.3
- pandas==2.2.3
Outras depend√™ncias est√£o listadas no arquivo `requirements_lang_chain.txt`, que pode ser instalado com o comando:
```bash
pip install -r requirements_lang_chain.txt
```

### Modelo LLM utilizado
Este projeto utiliza o <b>LLaMA 3.22</b> com <b>3 bilh√µes de par√¢metros</b>, instalado por meio da ferramenta Ollama. O Ollama √© necess√°rio para hospedar e interagir localmente com o modelo de linguagem.

Como instalar e rodar o modelo com Ollama:
Instale o Ollama conforme a documenta√ß√£o oficial: https://ollama.com/download

Ap√≥s a instala√ß√£o, execute o seguinte comando para baixar o modelo:
```bash
ollama run llama3
```
<b>Nota:</b> A instala√ß√£o do modelo pode levar alguns minutos e requer conex√£o com a internet.

# Preocupa√ß√µes com seguran√ßa

N√£o h√° preocupa√ß√µes conhecidas que possam prejudicar o ambiente dos avaliadores

# Instala√ß√£o
A seguir, apresentamos o passo a passo necess√°rio para instalar e executar a aplica√ß√£o localmente.

### 1. Instalar o Python 3
Certifique-se de que o Python 3 esteja instalado. Recomenda-se a vers√£o 3.10.
No Ubuntu, voc√™ pode instalar com:
```bash
sudo apt update
sudo apt install python3 python3-venv python3-pip
```

### 2. Criar e ativar o ambiente virtual
```bash
# Criar ambiente virtual
python3 -m venv forense-rag-logs-venv
cd forense-rag-logs-venv

# Ativar ambiente virtual
source bin/activate
```

### 3. Clonar o reposit√≥rio
```bash
git clone https://github.com/CGabriel22/forense-rag-logs
cd forense-rag-logs
```

### 4. Instalar as depend√™ncias
```bash
pip3 install -r requirements/requirements_lang_chain.txt
```

### 5. Instalar e configurar o Ollama
Para utilizar o modelo LLaMA localmente, √© necess√°rio instalar o Ollama:
Acesse https://ollama.com/download e siga as instru√ß√µes para seu sistema

Ap√≥s instalado, execute o comando abaixo para baixar o modelo LLaMA 3 com 3B de par√¢metros:
```bash
# Instala o modelo principal llama3.2
ollama run llama3.2
# Instala o modelo auxiliar gemma2:2b
ollama run gemma2:2b
```
Ap√≥s instalado, digite /bye para sair do modelo, instale o pr√≥ximo e repita o comando.
<b>Importante:</b> A primeira execu√ß√£o far√° o download do modelo e pode levar alguns minutos.

# Teste m√≠nimo

Para facilitar a avalia√ß√£o do artefato, disponibilizamos um notebook Jupyter com toda a estrutura de execu√ß√£o e um exemplo funcional ao final do arquivo.

### Passo a passo para execu√ß√£o:
1. Ative o ambiente virtual, caso ainda n√£o esteja ativo:
```bash
source forense-rag-logs/bin/activate
```

2. Acesse o diret√≥rio do projeto:
```bash
cd forense-rag-logs/src
```

3. Execute o Jupyter Notebook:
```bash
jupyter notebook
```
<b>Nota:</b> Pode ser necess√°rio instalar o jupyter lab antes pelo comando: `pip3 install jupyterlab`

4. Execute as c√©lulas sequencialmente.
Ao final do notebook, h√° uma c√©lula com um exemplo completo, utilizando os dados dispon√≠veis no reposit√≥rio para demonstrar o funcionamento do pipeline de an√°lise com RAG.
<b>Nota:</b> √â sugerido utilizar o vscode para melhor experi√™ncia com o notebook

### Observa√ß√µes:
- Todos os datasets utilizados nos testes est√£o inclu√≠dos no reposit√≥rio(eventlog.csv = 158k logs).
- A execu√ß√£o m√≠nima permite visualizar o fluxo completo: indexa√ß√£o, recupera√ß√£o, gera√ß√£o de resposta e an√°lise.
- Caso ocorra algum erro, revise a instala√ß√£o das depend√™ncias ou verifique se o modelo via Ollama est√° em execu√ß√£o.

# Experimentos

## üß™ Experimentos

Esta se√ß√£o descreve os experimentos realizados para validar as principais reivindica√ß√µes apresentadas no artigo. As abordagens aqui descritas foram implementadas e avaliadas de forma sistem√°tica, com anota√ß√µes manuais dos resultados, visando a verificar o impacto do uso de compress√£o contextual no desempenho de um sistema RAG aplicado √† an√°lise de logs de eventos.

### üîπ Reivindica√ß√£o #1: Gera√ß√£o estruturada de documentos a partir de logs

Para estruturar os dados de entrada, desenvolvemos uma fun√ß√£o de pr√©-processamento capaz de transformar cada linha do dataset de logs em um objeto `Document`, contendo tanto o conte√∫do em texto natural quanto metadados relevantes para posterior recupera√ß√£o. Essa etapa considerou campos como `MachineName`, `EntryType` e `TimeGenerated`, com tratamento de exce√ß√µes e logging para eventuais inconsist√™ncias.  

Essa estrutura permitiu que o pipeline de processamento subsequente operasse sobre um conjunto consistente de documentos com granularidade controlada.

### üîπ Reivindica√ß√£o #2: Embedding, armazenamento e recupera√ß√£o dos logs

Os documentos foram submetidos a uma etapa de segmenta√ß√£o (splitting) seguida de vetoriza√ß√£o com embeddings. O armazenamento foi realizado em uma base vetorial local. Utilizamos um `retriever` composto por diferentes estrat√©gias (ensemble) para maximizar a cobertura de recupera√ß√£o.

A experimenta√ß√£o dessa etapa consistiu em consultas simples com perguntas espec√≠ficas, como *"Qual m√°quina gerou o alerta de seguran√ßa?"*, e an√°lise qualitativa das respostas retornadas, verificando a relev√¢ncia do contexto selecionado.

### üîπ Reivindica√ß√£o #3: Compress√£o contextual com LLM

Uma das principais contribui√ß√µes deste trabalho foi a implementa√ß√£o de uma compress√£o contextual baseada em LLMs, utilizando o modelo `gemma2:2b` via Ollama. Foi desenvolvido um template de prompt com foco na extra√ß√£o de informa√ß√µes relevantes em cen√°rios de logs de seguran√ßa, com o seguinte formato:

```
Voc√™ √© um assistente especialista em an√°lise de logs de eventos de seguran√ßa. 
Dado o seguinte evento registrado no sistema, extraia as informa√ß√µes mais relevantes que respondam √† pergunta.

Pergunta: {question}

Registro de Evento:
{context}

Extra√ß√£o relevante:
```

O compressor foi integrado ao pipeline atrav√©s do `ContextualCompressionRetriever`, com o objetivo de reduzir a quantidade de informa√ß√£o irrelevante entregue ao modelo gerador.  

Os experimentos foram conduzidos de forma controlada: para cada pergunta, foi registrada a resposta com e sem compress√£o. As respostas foram ent√£o avaliadas manualmente quanto √† **precis√£o**, **concis√£o** e **clareza**, e categorizadas como satisfat√≥rias ou n√£o.

Ao final da recupera√ß√£o e compress√£o, a query final √© executada sobre o contexto reduzido.

---

### ‚öôÔ∏è Recursos Utilizados

- **Modelo LLM:** llama3.2, gemma2:2b (via Ollama local)
- **Dataset:** Logs sint√©ticos simulando eventos de seguran√ßa
- **Ambiente de execu√ß√£o:**  
  - CPU, 1GB RAM  
  - Execu√ß√£o local (sem GPU)
  - Tempo m√©dio por execu√ß√£o completa: **~15 minutos**
- **Ferramentas:** pandas, langchain, FAISS/Chroma, Ollama

---

# LICENSE
Esse projeto est√° licensiado sob a licensa GNU v3.0 conforme o arquivo de licensa: [LICENSE](https://github.com/CGabriel22/forense-rag-logs/blob/main/LICENSE)
